{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf798017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.metrics import Recall,AUC\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e70fd8",
   "metadata": {},
   "source": [
    "## 1. Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de73f9",
   "metadata": {},
   "source": [
    "### Non-Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859a767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_fire_img = glob.glob('/Users/samhumphries/Dropbox/Sams_Sanctuary/School/15.095/Project/fire_dataset/fire_images/*.png')\n",
    "lst_non_fire_img = glob.glob('/Users/samhumphries/Dropbox/Sams_Sanctuary/School/15.095/Project/fire_dataset/non_fire_images/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56bf590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               files  target\n",
       "0  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "1  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "2  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "3  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "4  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "5  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "6  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "7  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       0\n",
       "8  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       1\n",
       "9  /Users/samhumphries/Dropbox/Sams_Sanctuary/Sch...       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_fire = []\n",
    "for x in lst_fire_img:\n",
    "    lst_fire.append([x,1])\n",
    "lst_nn_fire = []\n",
    "for x in lst_non_fire_img:\n",
    "    lst_nn_fire.append([x,0])\n",
    "lst_complete = lst_fire + lst_nn_fire\n",
    "random.shuffle(lst_complete)\n",
    "\n",
    "df = pd.DataFrame(lst_complete,columns = ['files','target'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9704e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_img = '/kaggle/input/fire-dataset/fire_dataset/non_fire_images/non_fire.189.png'\n",
    "df = df.loc[~(df.loc[:,'files'] == filepath_img),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678f8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image(filepath):\n",
    "    img= cv2.imread(filepath) #read\n",
    "    img.shape\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR) #convert\n",
    "    img = cv2.resize(img,(256,256))  # resize\n",
    "    img = img / 255 #scale\n",
    "    return img \n",
    "\n",
    "def create_format_dataset(dataframe):\n",
    "    X = []\n",
    "    y = []\n",
    "    for f,t in dataframe.values:\n",
    "        try:\n",
    "            X.append(preprocessing_image(f))\n",
    "            y.append(t)\n",
    "        except:\n",
    "            pass\n",
    "  \n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1bed526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((998, 256, 256, 3), (998,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = create_format_dataset(df);\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0469f99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((798, 256, 256, 3), (200, 256, 256, 3), (798,), (200,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,stratify = y)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03055b",
   "metadata": {},
   "source": [
    "### Generator Way (*Ignore)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3d83aa1",
   "metadata": {},
   "source": [
    "#create an empty DataFrame\n",
    "df = pd.DataFrame(columns=['path','label'])\n",
    "\n",
    "#loop over fire images and label them 1\n",
    "for dirname, _, filenames in os.walk(\"/Users/samhumphries/Dropbox/Sams_Sanctuary/School/15.095/Project/fire_dataset/fire_images\"):\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        df = df.append(pd.DataFrame([[os.path.join(dirname, filename),'fire']],columns=['path','label']))\n",
    "\n",
    "#loop over non fire images and label them 0\n",
    "for dirname, _, filenames in os.walk(\"/Users/samhumphries/Dropbox/Sams_Sanctuary/School/15.095/Project/fire_dataset/non_fire_images\"):\n",
    "    for filename in filenames:\n",
    "        df = df.append(pd.DataFrame([[os.path.join(dirname, filename),'non_fire']],columns=['path','label']))\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "#shuffle the dataset for redistribute the labels\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eff512",
   "metadata": {},
   "source": [
    "### Data Generators"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a329d325",
   "metadata": {},
   "source": [
    "def shaper(row):\n",
    "    shape = image.load_img(row['path']).size\n",
    "    row['height'] = shape[1]\n",
    "    row['width'] = shape[0]\n",
    "    return row\n",
    "df = df.apply(shaper,axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ee2f91e",
   "metadata": {},
   "source": [
    "generator = ImageDataGenerator(\n",
    "    rotation_range= 20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range = 2,\n",
    "    zoom_range=0.2,\n",
    "    rescale = 1/255,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbf5941f",
   "metadata": {},
   "source": [
    "train_gen = generator.flow_from_dataframe(df,x_col='path',y_col='label',images_size=(256,256),class_mode='binary',subset='training')\n",
    "val_gen = generator.flow_from_dataframe(df,x_col='path',y_col='label',images_size=(256,256),class_mode='binary',subset='validation')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "687fddae",
   "metadata": {},
   "source": [
    "class_indices = {}\n",
    "for key in train_gen.class_indices.keys():\n",
    "    class_indices[train_gen.class_indices[key]] = key\n",
    "    \n",
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd2722",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c92d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size = (2,2),activation='relu',input_shape = (256,256,3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(filters=64,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(filters=128,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bdcc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 255, 255, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 126, 126, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 62, 62, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 123008)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                7872576   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,914,209\n",
      "Trainable params: 7,914,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6804c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stoppping = EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "646aef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.7061 - accuracy: 0.7427 - recall: 0.8068 - auc: 0.7430 - val_loss: 0.1121 - val_accuracy: 0.9600 - val_recall: 0.9868 - val_auc: 0.9906\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 24s 937ms/step - loss: 0.0946 - accuracy: 0.9609 - recall: 0.9782 - auc: 0.9922 - val_loss: 0.0959 - val_accuracy: 0.9600 - val_recall: 0.9934 - val_auc: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab6caa11c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss=\"binary_crossentropy\",metrics=['accuracy',Recall(),AUC()])\n",
    "\n",
    "\n",
    "#model.fit(train_gen, batch_size=32, epochs=2, validation_data=val_gen,callbacks=[early_stoppping,reduce_lr_on_plateau])\n",
    "model.fit(X_train,y_train, batch_size=32, epochs=2, validation_data=(X_test,y_test),callbacks=[early_stoppping,reduce_lr_on_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aa575cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123008, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[7].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73b542",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "287fa35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(Conv2D(filters=32,kernel_size = (2,2),activation='relu',input_shape = (256,256,3)))#,weights=model.layers[0].get_weights()))\n",
    "model2.add(MaxPool2D())\n",
    "model2.add(Conv2D(filters=64,kernel_size=(2,2),activation='relu'))#,weights=model.layers[2].get_weights()))\n",
    "model2.add(MaxPool2D())\n",
    "model2.add(Conv2D(filters=128,kernel_size=(2,2),activation='relu'))#,weights=model.layers[4].get_weights()))\n",
    "model2.add(MaxPool2D())\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64,activation=None))#,weights=model.layers[7].get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2141caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.layers[0].set_weights([model.layers[0].get_weights()[0],model.layers[0].get_weights()[1]])\n",
    "model2.layers[2].set_weights([model.layers[2].get_weights()[0],model.layers[2].get_weights()[1]])\n",
    "model2.layers[4].set_weights([model.layers[4].get_weights()[0],model.layers[4].get_weights()[1]])\n",
    "model2.layers[7].set_weights([model.layers[7].get_weights()[0],model.layers[7].get_weights()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38f16402",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = model2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c53a03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_test = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "986efca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aefdd028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 64) dtype=float32 (created by layer 'dense')>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[7].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a15834",
   "metadata": {},
   "source": [
    "### Test on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b434ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = activations\n",
    "y_train_emb = y_train\n",
    "\n",
    "X_test_emb = activations_test\n",
    "y_test_emb = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb295bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01dae7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhumphries/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "XGBoost = xgb.XGBClassifier(objective = \"binary:logistic\", random_state=42)\n",
    "XGBoost.fit(X_train_emb, y_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48ab5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = XGBoost.predict(X_test_emb)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b087932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test_emb, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b4e4e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce1b3ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter = 300)\n",
    "lr.fit(X_train_emb,y_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ec5c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr.predict(X_test_emb)\n",
    "accuracy = accuracy_score(y_test_emb, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63776c22",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "163c49fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train_emb,y_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe2f8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "y_pred=dt.predict(X_test_emb)\n",
    "accuracy = accuracy_score(y_test_emb, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23917239",
   "metadata": {},
   "source": [
    "### Holistic Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64df5c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samhumphries/Dropbox/Sams_Sanctuary/School/15.095/Project/HolisticDL/src\n"
     ]
    }
   ],
   "source": [
    "%cd \"HolisticDL/src\"\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6445071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samhumphries/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Batch Size: 32  ; stability subset ratio: 0.8  ; dropout value: 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/Sams_Sanctuary/School/15.095/Project/HolisticDL/src/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Set up data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Sams_Sanctuary/School/15.095/Project/HolisticDL/src/input_data.py\u001b[0m in \u001b[0;36mload_data_set\u001b[0;34m(training_size, validation_size, data_set, seed, reshape, dtype)\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;31m#Permute data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0mperm0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'X_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "%run train.py --batch_range 32 --network_size 256 128 --stab_ratio_range 0.8 --l2 1e-5 --data_set activations --train_size 0.8 --lr 3e-4 --val_size 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94bbeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samhumphries/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Batch Size: 64  ; stability subset ratio: 0.8  ; dropout value: 1\n",
      "(1468, 21)\n",
      "[0.43102831 0.43573095 0.43379643 0.43142807 0.42941474 0.43457447\n",
      " 0.43534686 0.4326187  0.42777773 0.43763041 0.43379643 0.4326187\n",
      " 0.47180438 0.46860346 0.47368115 0.47526599 0.46910821 0.46960847\n",
      " 0.47180438 0.47059556 0.47180438]\n",
      "[0.24659401 0.25476839 0.2513624  0.2472752  0.24386921 0.2527248\n",
      " 0.25408719 0.2493188  0.24114441 0.25817439 0.2513624  0.2493188\n",
      " 0.33446866 0.32561308 0.33991826 0.34468665 0.32697548 0.32833787\n",
      " 0.33446866 0.33106267 0.33446866]\n",
      "(940, 21)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[-6.77944698e-17  3.57633545e-16 -5.05505803e-17  7.53534351e-17\n",
      "  2.33383053e-16  2.11651028e-16  3.22791439e-16  8.24399650e-17\n",
      " -9.86208751e-17  2.77555756e-16 -4.67238541e-16 -5.66922396e-17\n",
      "  9.40854959e-16  2.93854775e-16 -1.33935416e-16 -8.48021417e-17\n",
      " -5.52749336e-17 -8.07864414e-17 -3.82672617e-17 -7.91329177e-17\n",
      "  9.67311337e-17]\n",
      "There are 940 samples in the training set.\n",
      "There are 234 samples in the validation set.\n",
      "WARNING:tensorflow:From /Users/samhumphries/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1cc7cdf70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1cc7cdf70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(1468, 21)\n",
      "[0.43102831 0.43573095 0.43379643 0.43142807 0.42941474 0.43457447\n",
      " 0.43534686 0.4326187  0.42777773 0.43763041 0.43379643 0.4326187\n",
      " 0.47180438 0.46860346 0.47368115 0.47526599 0.46910821 0.46960847\n",
      " 0.47180438 0.47059556 0.47180438]\n",
      "[0.24659401 0.25476839 0.2513624  0.2472752  0.24386921 0.2527248\n",
      " 0.25408719 0.2493188  0.24114441 0.25817439 0.2513624  0.2493188\n",
      " 0.33446866 0.32561308 0.33991826 0.34468665 0.32697548 0.32833787\n",
      " 0.33446866 0.33106267 0.33446866]\n",
      "(940, 21)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[-6.77944698e-17  3.57633545e-16 -5.05505803e-17  7.53534351e-17\n",
      "  2.33383053e-16  2.11651028e-16  3.22791439e-16  8.24399650e-17\n",
      " -9.86208751e-17  2.77555756e-16 -4.67238541e-16 -5.66922396e-17\n",
      "  9.40854959e-16  2.93854775e-16 -1.33935416e-16 -8.48021417e-17\n",
      " -5.52749336e-17 -8.07864414e-17 -3.82672617e-17 -7.91329177e-17\n",
      "  9.67311337e-17]\n",
      "There are 940 samples in the training set.\n",
      "There are 234 samples in the validation set.\n",
      "Step 0:    (2021-11-30 16:13:21.720838)\n",
      "    training nat accuracy 6.25\n",
      "    validation nat accuracy 3.419\n",
      "    Nat Xent 1.376\n",
      "    Stability Variable 0.1\n",
      "    Stable Xent 1.696\n",
      "    Non zero features percentage - W1 0.5\n",
      "    Non zero features percentage - W2 0.5006103515625\n",
      "    Non zero features percentage - W3 0.4765625\n",
      "    Regularizer 0.0\n",
      "Step 1000:    (2021-11-30 16:13:22.723454)\n",
      "    training nat accuracy 84.38\n",
      "    validation nat accuracy 87.18\n",
      "    Nat Xent 0.3777\n",
      "    Stability Variable 0.0008743\n",
      "    Stable Xent 0.4722\n",
      "    Non zero features percentage - W1 0.48474702380952384\n",
      "    Non zero features percentage - W2 0.52032470703125\n",
      "    Non zero features percentage - W3 0.474609375\n",
      "    Regularizer 0.0\n",
      "    76579.03254937123 examples per second\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1cc7cdca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1cc7cdca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(1468, 21)\n",
      "[0.43102831 0.43573095 0.43379643 0.43142807 0.42941474 0.43457447\n",
      " 0.43534686 0.4326187  0.42777773 0.43763041 0.43379643 0.4326187\n",
      " 0.47180438 0.46860346 0.47368115 0.47526599 0.46910821 0.46960847\n",
      " 0.47180438 0.47059556 0.47180438]\n",
      "[0.24659401 0.25476839 0.2513624  0.2472752  0.24386921 0.2527248\n",
      " 0.25408719 0.2493188  0.24114441 0.25817439 0.2513624  0.2493188\n",
      " 0.33446866 0.32561308 0.33991826 0.34468665 0.32697548 0.32833787\n",
      " 0.33446866 0.33106267 0.33446866]\n",
      "(940, 21)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[-3.66846033e-16  9.04713656e-17 -5.36214099e-17 -1.72202678e-16\n",
      "  3.03067264e-16 -2.60075649e-16 -5.83457632e-17  7.71841220e-16\n",
      "  1.70549154e-16  6.63771638e-17  1.26376451e-16 -2.10469939e-16\n",
      " -2.77555756e-17  2.43304195e-16 -2.06926674e-16 -8.38572710e-17\n",
      "  6.94479935e-17 -2.09997504e-16  9.47232836e-17  2.59839431e-18\n",
      "  1.86611955e-17]\n",
      "There are 940 samples in the training set.\n",
      "There are 234 samples in the validation set.\n",
      "Step 0:    (2021-11-30 16:13:25.027015)\n",
      "    training nat accuracy 3.125\n",
      "    validation nat accuracy 3.846\n",
      "    Nat Xent 1.381\n",
      "    Stability Variable 0.1\n",
      "    Stable Xent 1.701\n",
      "    Non zero features percentage - W1 0.5\n",
      "    Non zero features percentage - W2 0.5006103515625\n",
      "    Non zero features percentage - W3 0.4765625\n",
      "    Regularizer 0.0\n",
      "Step 1000:    (2021-11-30 16:13:26.032349)\n",
      "    training nat accuracy 90.62\n",
      "    validation nat accuracy 87.18\n",
      "    Nat Xent 0.2735\n",
      "    Stability Variable 0.0005477\n",
      "    Stable Xent 0.3419\n",
      "    Non zero features percentage - W1 0.48474702380952384\n",
      "    Non zero features percentage - W2 0.52239990234375\n",
      "    Non zero features percentage - W3 0.470703125\n",
      "    Regularizer 0.0\n",
      "    84088.41500045644 examples per second\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1ae2a0280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.__init__.<locals>.<lambda> at 0x7fd1ae2a0280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  Average testing accuracy 85.77\n",
      "  Individual accuracies: \n",
      " [84.61538553 86.92307472]\n",
      "  Adv testing accuracies {1e-05: array([0.83846152, 0.86538464]), 0.0001: array([0.81153846, 0.83076924]), 0.001: array([0.58461541, 0.56538463]), 0.01: array([0.22307692, 0.08461539]), 0.1: array([0.3576923 , 0.33076924])}\n",
      "  Stability values [0.0008743 0.0005477]\n",
      "  Test Accuracy std 1.2\n",
      "  Logits std 0.0138210217739066\n",
      "  Gini stability 0.0001923076923076923\n",
      "W1 std 0.02371642890006496\n",
      "W2 std 0.004542497834432564\n",
      "W3 std 0.007305313645701972\n",
      "W1 non zero percentage -0.009853325045846245\n",
      "W2 non zero percentage 0.0061560993588144\n",
      "W3 non zero percentage 0.0010690334854075445\n",
      "[2, 64, 0.8, 2000, 84.61538553237915, 86.92307472229004, 0.2499999998154938, 0.48474702380952384, 0.0, 0.0, 0.5213623046875, 0.0, 0.0, 0.47265625, 0.0, 0.0, 85.7692301273346, 1000.0, 0.8519230782985687, 0.821153849363327, 0.5750000178813934, 0.1538461558520794, 0.3442307710647583, True, 0, 0.8, 0, 0, [256, 128], 0.001, -0.009853325045846245, 0.0061560993588144, 0.0010690334854075445, 0.02371642890006496, 0.004542497834432564, 0.007305313645701972, 1.1538445949554443, array([0.02352509, 0.00302127, 0.02571645, 0.00302127]), 0.0001923076923076923]\n"
     ]
    }
   ],
   "source": [
    "%run train.py --data_set uci10 --is_stable --stab_ratio_range 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
